{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Rhz4tjGlrqL",
        "outputId": "3b9ec1fb-ddc0-49b0-951a-a10dd8cd18c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer \n",
        "from sklearn import decomposition, datasets\n",
        "from sklearn.svm import SVC, SVR\n",
        "import os, sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets\n",
        "from torchvision import transforms\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "pSwPfoE3my1_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "py_file_location = \"/content/drive/MyDrive/Machine_Learning/Data\"\n",
        "sys.path.append(os.path.abspath(py_file_location))"
      ],
      "metadata": {
        "id": "lpI92X1Rmzps"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import my_functions"
      ],
      "metadata": {
        "id": "4HP9jGe0m3Vt"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = '/content/drive/MyDrive/Machine_Learning/Data'"
      ],
      "metadata": {
        "id": "OLDBaWaPm7o4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "cifar10 = datasets.CIFAR10(\n",
        "    data_path, train=True, download=False,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))\n",
        "# Valadation\n",
        "cifar10_val = datasets.CIFAR10(\n",
        "    data_path, train=False, download=False,\n",
        "    transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
        "                             (0.2470, 0.2435, 0.2616))\n",
        "    ]))"
      ],
      "metadata": {
        "id": "7i2FpLNmm9wi"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "\n",
        "model_p1a = nn.Sequential(\n",
        "            nn.Linear(3072, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 10),\n",
        "            nn.LogSoftmax(dim=1))\n",
        "\n",
        "learning_rate = 1e-2\n",
        "\n",
        "optimizer = optim.SGD(model_p1a.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_fn = nn.NLLLoss()\n",
        "\n",
        "n_epochs = 300\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = model_p1a(imgs.view(imgs.shape[0], -1))\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okcvio0mnGQZ",
        "outputId": "86d31ebd-be80-4a7f-ea21-18bedcbae8ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 1.277668\n",
            "Epoch: 1, Loss: 1.733036\n",
            "Epoch: 2, Loss: 1.438715\n",
            "Epoch: 3, Loss: 1.648282\n",
            "Epoch: 4, Loss: 1.726078\n",
            "Epoch: 5, Loss: 1.880649\n",
            "Epoch: 6, Loss: 1.885775\n",
            "Epoch: 7, Loss: 1.670855\n",
            "Epoch: 8, Loss: 1.359705\n",
            "Epoch: 9, Loss: 1.243368\n",
            "Epoch: 10, Loss: 1.564360\n",
            "Epoch: 11, Loss: 1.308309\n",
            "Epoch: 12, Loss: 0.763536\n",
            "Epoch: 13, Loss: 1.098985\n",
            "Epoch: 14, Loss: 1.153894\n",
            "Epoch: 15, Loss: 0.660009\n",
            "Epoch: 16, Loss: 1.494898\n",
            "Epoch: 17, Loss: 1.419065\n",
            "Epoch: 18, Loss: 1.040358\n",
            "Epoch: 19, Loss: 0.980775\n",
            "Epoch: 20, Loss: 0.754087\n",
            "Epoch: 21, Loss: 1.029852\n",
            "Epoch: 22, Loss: 1.180610\n",
            "Epoch: 23, Loss: 0.837711\n",
            "Epoch: 24, Loss: 1.058903\n",
            "Epoch: 25, Loss: 1.149859\n",
            "Epoch: 26, Loss: 1.006525\n",
            "Epoch: 27, Loss: 0.936198\n",
            "Epoch: 28, Loss: 0.496978\n",
            "Epoch: 29, Loss: 0.480132\n",
            "Epoch: 30, Loss: 0.685004\n",
            "Epoch: 31, Loss: 0.991130\n",
            "Epoch: 32, Loss: 0.578945\n",
            "Epoch: 33, Loss: 0.362616\n",
            "Epoch: 34, Loss: 0.470177\n",
            "Epoch: 35, Loss: 0.405472\n",
            "Epoch: 36, Loss: 0.511212\n",
            "Epoch: 37, Loss: 0.527839\n",
            "Epoch: 38, Loss: 0.326950\n",
            "Epoch: 39, Loss: 0.333585\n",
            "Epoch: 40, Loss: 0.401667\n",
            "Epoch: 41, Loss: 0.304718\n",
            "Epoch: 42, Loss: 0.526168\n",
            "Epoch: 43, Loss: 0.686259\n",
            "Epoch: 44, Loss: 0.417267\n",
            "Epoch: 45, Loss: 0.522825\n",
            "Epoch: 46, Loss: 0.350590\n",
            "Epoch: 47, Loss: 0.178540\n",
            "Epoch: 48, Loss: 0.445727\n",
            "Epoch: 49, Loss: 0.308105\n",
            "Epoch: 50, Loss: 0.274853\n",
            "Epoch: 51, Loss: 0.317215\n",
            "Epoch: 52, Loss: 0.356994\n",
            "Epoch: 53, Loss: 0.195996\n",
            "Epoch: 54, Loss: 0.071031\n",
            "Epoch: 55, Loss: 0.124839\n",
            "Epoch: 56, Loss: 0.316809\n",
            "Epoch: 57, Loss: 0.112511\n",
            "Epoch: 58, Loss: 0.194849\n",
            "Epoch: 59, Loss: 0.205707\n",
            "Epoch: 60, Loss: 0.109454\n",
            "Epoch: 61, Loss: 0.119190\n",
            "Epoch: 62, Loss: 0.215742\n",
            "Epoch: 63, Loss: 0.185933\n",
            "Epoch: 64, Loss: 0.117883\n",
            "Epoch: 65, Loss: 0.109988\n",
            "Epoch: 66, Loss: 0.200621\n",
            "Epoch: 67, Loss: 0.298537\n",
            "Epoch: 68, Loss: 0.159129\n",
            "Epoch: 69, Loss: 0.096140\n",
            "Epoch: 70, Loss: 0.071370\n",
            "Epoch: 71, Loss: 0.145896\n",
            "Epoch: 72, Loss: 0.088284\n",
            "Epoch: 73, Loss: 0.053685\n",
            "Epoch: 74, Loss: 0.079522\n",
            "Epoch: 75, Loss: 0.071858\n",
            "Epoch: 76, Loss: 0.038572\n",
            "Epoch: 77, Loss: 0.159605\n",
            "Epoch: 78, Loss: 0.065019\n",
            "Epoch: 79, Loss: 0.063855\n",
            "Epoch: 80, Loss: 0.042252\n",
            "Epoch: 81, Loss: 0.034119\n",
            "Epoch: 82, Loss: 0.082301\n",
            "Epoch: 83, Loss: 0.084222\n",
            "Epoch: 84, Loss: 0.135124\n",
            "Epoch: 85, Loss: 0.093777\n",
            "Epoch: 86, Loss: 0.065863\n",
            "Epoch: 87, Loss: 0.100533\n",
            "Epoch: 88, Loss: 0.062480\n",
            "Epoch: 89, Loss: 0.074528\n",
            "Epoch: 90, Loss: 0.063991\n",
            "Epoch: 91, Loss: 0.041236\n",
            "Epoch: 92, Loss: 0.081333\n",
            "Epoch: 93, Loss: 0.067651\n",
            "Epoch: 94, Loss: 0.061799\n",
            "Epoch: 95, Loss: 0.047646\n",
            "Epoch: 96, Loss: 0.047611\n",
            "Epoch: 97, Loss: 0.034320\n",
            "Epoch: 98, Loss: 0.070614\n",
            "Epoch: 99, Loss: 0.038968\n",
            "Epoch: 100, Loss: 0.043787\n",
            "Epoch: 101, Loss: 0.046014\n",
            "Epoch: 102, Loss: 0.041650\n",
            "Epoch: 103, Loss: 0.046530\n",
            "Epoch: 104, Loss: 0.038036\n",
            "Epoch: 105, Loss: 0.040385\n",
            "Epoch: 106, Loss: 0.050420\n",
            "Epoch: 107, Loss: 0.022034\n",
            "Epoch: 108, Loss: 0.036527\n",
            "Epoch: 109, Loss: 0.032933\n",
            "Epoch: 110, Loss: 0.020663\n",
            "Epoch: 111, Loss: 0.049782\n",
            "Epoch: 112, Loss: 0.041904\n",
            "Epoch: 113, Loss: 0.047127\n",
            "Epoch: 114, Loss: 0.030090\n",
            "Epoch: 115, Loss: 0.061479\n",
            "Epoch: 116, Loss: 0.031228\n",
            "Epoch: 117, Loss: 0.043644\n",
            "Epoch: 118, Loss: 0.037321\n",
            "Epoch: 119, Loss: 0.035897\n",
            "Epoch: 120, Loss: 0.020000\n",
            "Epoch: 121, Loss: 0.021598\n",
            "Epoch: 122, Loss: 0.020947\n",
            "Epoch: 123, Loss: 0.017640\n",
            "Epoch: 124, Loss: 0.025517\n",
            "Epoch: 125, Loss: 0.030025\n",
            "Epoch: 126, Loss: 0.027019\n",
            "Epoch: 127, Loss: 0.026463\n",
            "Epoch: 128, Loss: 0.021581\n",
            "Epoch: 129, Loss: 0.023511\n",
            "Epoch: 130, Loss: 0.020652\n",
            "Epoch: 131, Loss: 0.023004\n",
            "Epoch: 132, Loss: 0.021043\n",
            "Epoch: 133, Loss: 0.029759\n",
            "Epoch: 134, Loss: 0.054466\n",
            "Epoch: 135, Loss: 0.024079\n",
            "Epoch: 136, Loss: 0.018698\n",
            "Epoch: 137, Loss: 0.030052\n",
            "Epoch: 138, Loss: 0.010663\n",
            "Epoch: 139, Loss: 0.018297\n",
            "Epoch: 140, Loss: 0.021002\n",
            "Epoch: 141, Loss: 0.011693\n",
            "Epoch: 142, Loss: 0.016253\n",
            "Epoch: 143, Loss: 0.015355\n",
            "Epoch: 144, Loss: 0.012065\n",
            "Epoch: 145, Loss: 0.021679\n",
            "Epoch: 146, Loss: 0.011184\n",
            "Epoch: 147, Loss: 0.019873\n",
            "Epoch: 148, Loss: 0.018793\n",
            "Epoch: 149, Loss: 0.039853\n",
            "Epoch: 150, Loss: 0.019071\n",
            "Epoch: 151, Loss: 0.017442\n",
            "Epoch: 152, Loss: 0.014320\n",
            "Epoch: 153, Loss: 0.015248\n",
            "Epoch: 154, Loss: 0.011534\n",
            "Epoch: 155, Loss: 0.019124\n",
            "Epoch: 156, Loss: 0.014686\n",
            "Epoch: 157, Loss: 0.016351\n",
            "Epoch: 158, Loss: 0.025094\n",
            "Epoch: 159, Loss: 0.016417\n",
            "Epoch: 160, Loss: 0.014760\n",
            "Epoch: 161, Loss: 0.009930\n",
            "Epoch: 162, Loss: 0.018708\n",
            "Epoch: 163, Loss: 0.016528\n",
            "Epoch: 164, Loss: 0.015307\n",
            "Epoch: 165, Loss: 0.016150\n",
            "Epoch: 166, Loss: 0.007971\n",
            "Epoch: 167, Loss: 0.020036\n",
            "Epoch: 168, Loss: 0.013554\n",
            "Epoch: 169, Loss: 0.013922\n",
            "Epoch: 170, Loss: 0.018229\n",
            "Epoch: 171, Loss: 0.012157\n",
            "Epoch: 172, Loss: 0.023736\n",
            "Epoch: 173, Loss: 0.020091\n",
            "Epoch: 174, Loss: 0.009552\n",
            "Epoch: 175, Loss: 0.012964\n",
            "Epoch: 176, Loss: 0.017612\n",
            "Epoch: 177, Loss: 0.016706\n",
            "Epoch: 178, Loss: 0.017676\n",
            "Epoch: 179, Loss: 0.009979\n",
            "Epoch: 180, Loss: 0.009651\n",
            "Epoch: 181, Loss: 0.009780\n",
            "Epoch: 182, Loss: 0.013873\n",
            "Epoch: 183, Loss: 0.008037\n",
            "Epoch: 184, Loss: 0.012218\n",
            "Epoch: 185, Loss: 0.015157\n",
            "Epoch: 186, Loss: 0.010152\n",
            "Epoch: 187, Loss: 0.019958\n",
            "Epoch: 188, Loss: 0.010115\n",
            "Epoch: 189, Loss: 0.014938\n",
            "Epoch: 190, Loss: 0.015502\n",
            "Epoch: 191, Loss: 0.009227\n",
            "Epoch: 192, Loss: 0.005238\n",
            "Epoch: 193, Loss: 0.009722\n",
            "Epoch: 194, Loss: 0.007200\n",
            "Epoch: 195, Loss: 0.009561\n",
            "Epoch: 196, Loss: 0.015060\n",
            "Epoch: 197, Loss: 0.011319\n",
            "Epoch: 198, Loss: 0.008644\n",
            "Epoch: 199, Loss: 0.022297\n",
            "Epoch: 200, Loss: 0.014787\n",
            "Epoch: 201, Loss: 0.006780\n",
            "Epoch: 202, Loss: 0.011620\n",
            "Epoch: 203, Loss: 0.010140\n",
            "Epoch: 204, Loss: 0.017338\n",
            "Epoch: 205, Loss: 0.007043\n",
            "Epoch: 206, Loss: 0.016756\n",
            "Epoch: 207, Loss: 0.013424\n",
            "Epoch: 208, Loss: 0.006447\n",
            "Epoch: 209, Loss: 0.005874\n",
            "Epoch: 210, Loss: 0.009412\n",
            "Epoch: 211, Loss: 0.008912\n",
            "Epoch: 212, Loss: 0.010251\n",
            "Epoch: 213, Loss: 0.012989\n",
            "Epoch: 214, Loss: 0.009930\n",
            "Epoch: 215, Loss: 0.008482\n",
            "Epoch: 216, Loss: 0.015507\n",
            "Epoch: 217, Loss: 0.019596\n",
            "Epoch: 218, Loss: 0.010328\n",
            "Epoch: 219, Loss: 0.005985\n",
            "Epoch: 220, Loss: 0.016735\n",
            "Epoch: 221, Loss: 0.007898\n",
            "Epoch: 222, Loss: 0.009028\n",
            "Epoch: 223, Loss: 0.008784\n",
            "Epoch: 224, Loss: 0.010059\n",
            "Epoch: 225, Loss: 0.007075\n",
            "Epoch: 226, Loss: 0.009159\n",
            "Epoch: 227, Loss: 0.009104\n",
            "Epoch: 228, Loss: 0.005918\n",
            "Epoch: 229, Loss: 0.005054\n",
            "Epoch: 230, Loss: 0.007153\n",
            "Epoch: 231, Loss: 0.008916\n",
            "Epoch: 232, Loss: 0.009894\n",
            "Epoch: 233, Loss: 0.012036\n",
            "Epoch: 234, Loss: 0.009238\n",
            "Epoch: 235, Loss: 0.013441\n",
            "Epoch: 236, Loss: 0.007302\n",
            "Epoch: 237, Loss: 0.009747\n",
            "Epoch: 238, Loss: 0.009151\n",
            "Epoch: 239, Loss: 0.008118\n",
            "Epoch: 240, Loss: 0.007729\n",
            "Epoch: 241, Loss: 0.008463\n",
            "Epoch: 242, Loss: 0.010459\n",
            "Epoch: 243, Loss: 0.008315\n",
            "Epoch: 244, Loss: 0.008306\n",
            "Epoch: 245, Loss: 0.006406\n",
            "Epoch: 246, Loss: 0.006971\n",
            "Epoch: 247, Loss: 0.005953\n",
            "Epoch: 248, Loss: 0.007423\n",
            "Epoch: 249, Loss: 0.006615\n",
            "Epoch: 250, Loss: 0.012353\n",
            "Epoch: 251, Loss: 0.007670\n",
            "Epoch: 252, Loss: 0.008461\n",
            "Epoch: 253, Loss: 0.008985\n",
            "Epoch: 254, Loss: 0.005268\n",
            "Epoch: 255, Loss: 0.006550\n",
            "Epoch: 256, Loss: 0.010164\n",
            "Epoch: 257, Loss: 0.006509\n",
            "Epoch: 258, Loss: 0.005225\n",
            "Epoch: 259, Loss: 0.007837\n",
            "Epoch: 260, Loss: 0.007314\n",
            "Epoch: 261, Loss: 0.005877\n",
            "Epoch: 262, Loss: 0.005115\n",
            "Epoch: 263, Loss: 0.006589\n",
            "Epoch: 264, Loss: 0.007373\n",
            "Epoch: 265, Loss: 0.008603\n",
            "Epoch: 266, Loss: 0.005372\n",
            "Epoch: 267, Loss: 0.007283\n",
            "Epoch: 268, Loss: 0.007391\n",
            "Epoch: 269, Loss: 0.005501\n",
            "Epoch: 270, Loss: 0.010686\n",
            "Epoch: 271, Loss: 0.005766\n",
            "Epoch: 272, Loss: 0.009775\n",
            "Epoch: 273, Loss: 0.009677\n",
            "Epoch: 274, Loss: 0.005309\n",
            "Epoch: 275, Loss: 0.005439\n",
            "Epoch: 276, Loss: 0.003843\n",
            "Epoch: 277, Loss: 0.006408\n",
            "Epoch: 278, Loss: 0.006754\n",
            "Epoch: 279, Loss: 0.010722\n",
            "Epoch: 280, Loss: 0.006636\n",
            "Epoch: 281, Loss: 0.007326\n",
            "Epoch: 282, Loss: 0.005782\n",
            "Epoch: 283, Loss: 0.006020\n",
            "Epoch: 284, Loss: 0.003329\n",
            "Epoch: 285, Loss: 0.005636\n",
            "Epoch: 286, Loss: 0.007686\n",
            "Epoch: 287, Loss: 0.006960\n",
            "Epoch: 288, Loss: 0.004276\n",
            "Epoch: 289, Loss: 0.005811\n",
            "Epoch: 290, Loss: 0.011409\n",
            "Epoch: 291, Loss: 0.009207\n",
            "Epoch: 292, Loss: 0.005880\n",
            "Epoch: 293, Loss: 0.005299\n",
            "Epoch: 294, Loss: 0.005989\n",
            "Epoch: 295, Loss: 0.005869\n",
            "Epoch: 296, Loss: 0.005917\n",
            "Epoch: 297, Loss: 0.011773\n",
            "Epoch: 298, Loss: 0.008093\n",
            "Epoch: 299, Loss: 0.007315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_p1a.state_dict(), data_path + '/problem_1a_model.pt')"
      ],
      "metadata": {
        "id": "KZ2zKFltnNe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = model_p1a\n",
        "loaded_model.load_state_dict(torch.load(data_path + 'problem_1a_model.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2P8IMWXN73YA",
        "outputId": "2b10a2dd-afbd-4790-ffaa-1c8877dac175"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = model_p1a(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "        \n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pWoJIhmKnO93",
        "outputId": "05aa6dd7-7648-447d-f0f5-aa7b40d10e03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = model_p1a(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xi3jh1ep-3qD",
        "outputId": "f4becaba-89bc-43d1-b2e1-43ceda2f3ecc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.467700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Increasing Complexity \n",
        "\n",
        "train_loader2 = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "\n",
        "model_p1b = nn.Sequential(\n",
        "            nn.Linear(3072, 1024),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(1024, 512),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(512, 128),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(128,10),\n",
        "            nn.LogSoftmax(dim=1))\n",
        "\n",
        "learning_rate = 1e-2\n",
        "\n",
        "optimizer = optim.SGD(model_p1b.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "n_epochs = 300\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for imgs, labels in train_loader2:\n",
        "        outputs = model_p1b(imgs.view(imgs.shape[0], -1))\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6mg5SOnnQ_0",
        "outputId": "d1ca30c7-efd1-4e3d-b745-0cb2317be7f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 1.752347\n",
            "Epoch: 1, Loss: 1.830350\n",
            "Epoch: 2, Loss: 1.444305\n",
            "Epoch: 3, Loss: 1.683247\n",
            "Epoch: 4, Loss: 2.100591\n",
            "Epoch: 5, Loss: 1.796864\n",
            "Epoch: 6, Loss: 1.309047\n",
            "Epoch: 7, Loss: 1.430395\n",
            "Epoch: 8, Loss: 1.200429\n",
            "Epoch: 9, Loss: 0.774779\n",
            "Epoch: 10, Loss: 1.399044\n",
            "Epoch: 11, Loss: 1.365424\n",
            "Epoch: 12, Loss: 1.353392\n",
            "Epoch: 13, Loss: 1.235602\n",
            "Epoch: 14, Loss: 1.725698\n",
            "Epoch: 15, Loss: 1.050238\n",
            "Epoch: 16, Loss: 2.222200\n",
            "Epoch: 17, Loss: 1.062057\n",
            "Epoch: 18, Loss: 1.443056\n",
            "Epoch: 19, Loss: 1.135771\n",
            "Epoch: 20, Loss: 1.107488\n",
            "Epoch: 21, Loss: 0.641490\n",
            "Epoch: 22, Loss: 0.756833\n",
            "Epoch: 23, Loss: 0.403828\n",
            "Epoch: 24, Loss: 1.166624\n",
            "Epoch: 25, Loss: 1.283652\n",
            "Epoch: 26, Loss: 0.497824\n",
            "Epoch: 27, Loss: 0.440657\n",
            "Epoch: 28, Loss: 0.290123\n",
            "Epoch: 29, Loss: 0.341790\n",
            "Epoch: 30, Loss: 0.377572\n",
            "Epoch: 31, Loss: 0.358356\n",
            "Epoch: 32, Loss: 0.377631\n",
            "Epoch: 33, Loss: 0.067828\n",
            "Epoch: 34, Loss: 0.238089\n",
            "Epoch: 35, Loss: 0.117105\n",
            "Epoch: 36, Loss: 0.472049\n",
            "Epoch: 37, Loss: 0.319731\n",
            "Epoch: 38, Loss: 0.164315\n",
            "Epoch: 39, Loss: 0.434745\n",
            "Epoch: 40, Loss: 0.031019\n",
            "Epoch: 41, Loss: 0.057401\n",
            "Epoch: 42, Loss: 0.064095\n",
            "Epoch: 43, Loss: 0.067039\n",
            "Epoch: 44, Loss: 0.127655\n",
            "Epoch: 45, Loss: 0.426050\n",
            "Epoch: 46, Loss: 0.087861\n",
            "Epoch: 47, Loss: 0.050914\n",
            "Epoch: 48, Loss: 0.020547\n",
            "Epoch: 49, Loss: 0.010942\n",
            "Epoch: 50, Loss: 0.111238\n",
            "Epoch: 51, Loss: 0.078386\n",
            "Epoch: 52, Loss: 0.008878\n",
            "Epoch: 53, Loss: 0.128387\n",
            "Epoch: 54, Loss: 0.018200\n",
            "Epoch: 55, Loss: 0.018790\n",
            "Epoch: 56, Loss: 0.008594\n",
            "Epoch: 57, Loss: 0.010979\n",
            "Epoch: 58, Loss: 0.028312\n",
            "Epoch: 59, Loss: 0.022075\n",
            "Epoch: 60, Loss: 0.096211\n",
            "Epoch: 61, Loss: 0.024948\n",
            "Epoch: 62, Loss: 0.007319\n",
            "Epoch: 63, Loss: 0.003125\n",
            "Epoch: 64, Loss: 0.005688\n",
            "Epoch: 65, Loss: 0.004985\n",
            "Epoch: 66, Loss: 0.003215\n",
            "Epoch: 67, Loss: 0.002813\n",
            "Epoch: 68, Loss: 0.003056\n",
            "Epoch: 69, Loss: 0.004346\n",
            "Epoch: 70, Loss: 0.001497\n",
            "Epoch: 71, Loss: 0.004155\n",
            "Epoch: 72, Loss: 0.002777\n",
            "Epoch: 73, Loss: 0.002732\n",
            "Epoch: 74, Loss: 0.002961\n",
            "Epoch: 75, Loss: 0.001509\n",
            "Epoch: 76, Loss: 0.002784\n",
            "Epoch: 77, Loss: 0.002011\n",
            "Epoch: 78, Loss: 0.000910\n",
            "Epoch: 79, Loss: 0.001436\n",
            "Epoch: 80, Loss: 0.001335\n",
            "Epoch: 81, Loss: 0.002814\n",
            "Epoch: 82, Loss: 0.002054\n",
            "Epoch: 83, Loss: 0.002296\n",
            "Epoch: 84, Loss: 0.001552\n",
            "Epoch: 85, Loss: 0.000910\n",
            "Epoch: 86, Loss: 0.001213\n",
            "Epoch: 87, Loss: 0.002308\n",
            "Epoch: 88, Loss: 0.001330\n",
            "Epoch: 89, Loss: 0.001414\n",
            "Epoch: 90, Loss: 0.001098\n",
            "Epoch: 91, Loss: 0.001055\n",
            "Epoch: 92, Loss: 0.000875\n",
            "Epoch: 93, Loss: 0.002053\n",
            "Epoch: 94, Loss: 0.000931\n",
            "Epoch: 95, Loss: 0.000986\n",
            "Epoch: 96, Loss: 0.000900\n",
            "Epoch: 97, Loss: 0.001150\n",
            "Epoch: 98, Loss: 0.001687\n",
            "Epoch: 99, Loss: 0.001833\n",
            "Epoch: 100, Loss: 0.001410\n",
            "Epoch: 101, Loss: 0.001233\n",
            "Epoch: 102, Loss: 0.001740\n",
            "Epoch: 103, Loss: 0.001240\n",
            "Epoch: 104, Loss: 0.001758\n",
            "Epoch: 105, Loss: 0.001489\n",
            "Epoch: 106, Loss: 0.000586\n",
            "Epoch: 107, Loss: 0.000455\n",
            "Epoch: 108, Loss: 0.000562\n",
            "Epoch: 109, Loss: 0.000752\n",
            "Epoch: 110, Loss: 0.000966\n",
            "Epoch: 111, Loss: 0.001027\n",
            "Epoch: 112, Loss: 0.001256\n",
            "Epoch: 113, Loss: 0.000883\n",
            "Epoch: 114, Loss: 0.001210\n",
            "Epoch: 115, Loss: 0.001114\n",
            "Epoch: 116, Loss: 0.000674\n",
            "Epoch: 117, Loss: 0.001658\n",
            "Epoch: 118, Loss: 0.000567\n",
            "Epoch: 119, Loss: 0.000777\n",
            "Epoch: 120, Loss: 0.001193\n",
            "Epoch: 121, Loss: 0.001070\n",
            "Epoch: 122, Loss: 0.000867\n",
            "Epoch: 123, Loss: 0.000551\n",
            "Epoch: 124, Loss: 0.000492\n",
            "Epoch: 125, Loss: 0.000718\n",
            "Epoch: 126, Loss: 0.001079\n",
            "Epoch: 127, Loss: 0.000440\n",
            "Epoch: 128, Loss: 0.000654\n",
            "Epoch: 129, Loss: 0.000843\n",
            "Epoch: 130, Loss: 0.000839\n",
            "Epoch: 131, Loss: 0.000651\n",
            "Epoch: 132, Loss: 0.000554\n",
            "Epoch: 133, Loss: 0.000724\n",
            "Epoch: 134, Loss: 0.000460\n",
            "Epoch: 135, Loss: 0.001071\n",
            "Epoch: 136, Loss: 0.001040\n",
            "Epoch: 137, Loss: 0.000646\n",
            "Epoch: 138, Loss: 0.000228\n",
            "Epoch: 139, Loss: 0.000724\n",
            "Epoch: 140, Loss: 0.000699\n",
            "Epoch: 141, Loss: 0.000603\n",
            "Epoch: 142, Loss: 0.000309\n",
            "Epoch: 143, Loss: 0.000973\n",
            "Epoch: 144, Loss: 0.000496\n",
            "Epoch: 145, Loss: 0.000451\n",
            "Epoch: 146, Loss: 0.000415\n",
            "Epoch: 147, Loss: 0.000643\n",
            "Epoch: 148, Loss: 0.000891\n",
            "Epoch: 149, Loss: 0.000620\n",
            "Epoch: 150, Loss: 0.000617\n",
            "Epoch: 151, Loss: 0.000385\n",
            "Epoch: 152, Loss: 0.000256\n",
            "Epoch: 153, Loss: 0.000388\n",
            "Epoch: 154, Loss: 0.000772\n",
            "Epoch: 155, Loss: 0.000762\n",
            "Epoch: 156, Loss: 0.000491\n",
            "Epoch: 157, Loss: 0.000453\n",
            "Epoch: 158, Loss: 0.000474\n",
            "Epoch: 159, Loss: 0.000491\n",
            "Epoch: 160, Loss: 0.000300\n",
            "Epoch: 161, Loss: 0.000787\n",
            "Epoch: 162, Loss: 0.000425\n",
            "Epoch: 163, Loss: 0.000506\n",
            "Epoch: 164, Loss: 0.000704\n",
            "Epoch: 165, Loss: 0.000416\n",
            "Epoch: 166, Loss: 0.000616\n",
            "Epoch: 167, Loss: 0.000164\n",
            "Epoch: 168, Loss: 0.000788\n",
            "Epoch: 169, Loss: 0.000414\n",
            "Epoch: 170, Loss: 0.000785\n",
            "Epoch: 171, Loss: 0.000583\n",
            "Epoch: 172, Loss: 0.000781\n",
            "Epoch: 173, Loss: 0.000417\n",
            "Epoch: 174, Loss: 0.000412\n",
            "Epoch: 175, Loss: 0.000534\n",
            "Epoch: 176, Loss: 0.000627\n",
            "Epoch: 177, Loss: 0.000458\n",
            "Epoch: 178, Loss: 0.000409\n",
            "Epoch: 179, Loss: 0.000817\n",
            "Epoch: 180, Loss: 0.000302\n",
            "Epoch: 181, Loss: 0.000327\n",
            "Epoch: 182, Loss: 0.000457\n",
            "Epoch: 183, Loss: 0.000303\n",
            "Epoch: 184, Loss: 0.000416\n",
            "Epoch: 185, Loss: 0.000852\n",
            "Epoch: 186, Loss: 0.000269\n",
            "Epoch: 187, Loss: 0.000403\n",
            "Epoch: 188, Loss: 0.000399\n",
            "Epoch: 189, Loss: 0.000242\n",
            "Epoch: 190, Loss: 0.000328\n",
            "Epoch: 191, Loss: 0.000253\n",
            "Epoch: 192, Loss: 0.000452\n",
            "Epoch: 193, Loss: 0.000683\n",
            "Epoch: 194, Loss: 0.000391\n",
            "Epoch: 195, Loss: 0.000359\n",
            "Epoch: 196, Loss: 0.000333\n",
            "Epoch: 197, Loss: 0.000411\n",
            "Epoch: 198, Loss: 0.000284\n",
            "Epoch: 199, Loss: 0.000616\n",
            "Epoch: 200, Loss: 0.000635\n",
            "Epoch: 201, Loss: 0.000874\n",
            "Epoch: 202, Loss: 0.000388\n",
            "Epoch: 203, Loss: 0.000427\n",
            "Epoch: 204, Loss: 0.000405\n",
            "Epoch: 205, Loss: 0.000447\n",
            "Epoch: 206, Loss: 0.000217\n",
            "Epoch: 207, Loss: 0.000265\n",
            "Epoch: 208, Loss: 0.000223\n",
            "Epoch: 209, Loss: 0.000416\n",
            "Epoch: 210, Loss: 0.000471\n",
            "Epoch: 211, Loss: 0.000367\n",
            "Epoch: 212, Loss: 0.000181\n",
            "Epoch: 213, Loss: 0.000574\n",
            "Epoch: 214, Loss: 0.000314\n",
            "Epoch: 215, Loss: 0.000424\n",
            "Epoch: 216, Loss: 0.000248\n",
            "Epoch: 217, Loss: 0.000448\n",
            "Epoch: 218, Loss: 0.000329\n",
            "Epoch: 219, Loss: 0.000403\n",
            "Epoch: 220, Loss: 0.000460\n",
            "Epoch: 221, Loss: 0.000322\n",
            "Epoch: 222, Loss: 0.000313\n",
            "Epoch: 223, Loss: 0.000190\n",
            "Epoch: 224, Loss: 0.000348\n",
            "Epoch: 225, Loss: 0.000322\n",
            "Epoch: 226, Loss: 0.000428\n",
            "Epoch: 227, Loss: 0.000291\n",
            "Epoch: 228, Loss: 0.000425\n",
            "Epoch: 229, Loss: 0.000434\n",
            "Epoch: 230, Loss: 0.000225\n",
            "Epoch: 231, Loss: 0.000180\n",
            "Epoch: 232, Loss: 0.000405\n",
            "Epoch: 233, Loss: 0.000308\n",
            "Epoch: 234, Loss: 0.000547\n",
            "Epoch: 235, Loss: 0.000344\n",
            "Epoch: 236, Loss: 0.000282\n",
            "Epoch: 237, Loss: 0.000325\n",
            "Epoch: 238, Loss: 0.000092\n",
            "Epoch: 239, Loss: 0.000225\n",
            "Epoch: 240, Loss: 0.000256\n",
            "Epoch: 241, Loss: 0.000216\n",
            "Epoch: 242, Loss: 0.000413\n",
            "Epoch: 243, Loss: 0.000164\n",
            "Epoch: 244, Loss: 0.000255\n",
            "Epoch: 245, Loss: 0.000335\n",
            "Epoch: 246, Loss: 0.000282\n",
            "Epoch: 247, Loss: 0.000399\n",
            "Epoch: 248, Loss: 0.000241\n",
            "Epoch: 249, Loss: 0.000194\n",
            "Epoch: 250, Loss: 0.000158\n",
            "Epoch: 251, Loss: 0.000279\n",
            "Epoch: 252, Loss: 0.000301\n",
            "Epoch: 253, Loss: 0.000379\n",
            "Epoch: 254, Loss: 0.000169\n",
            "Epoch: 255, Loss: 0.000255\n",
            "Epoch: 256, Loss: 0.000202\n",
            "Epoch: 257, Loss: 0.000348\n",
            "Epoch: 258, Loss: 0.000332\n",
            "Epoch: 259, Loss: 0.000261\n",
            "Epoch: 260, Loss: 0.000346\n",
            "Epoch: 261, Loss: 0.000367\n",
            "Epoch: 262, Loss: 0.000313\n",
            "Epoch: 263, Loss: 0.000499\n",
            "Epoch: 264, Loss: 0.000415\n",
            "Epoch: 265, Loss: 0.000207\n",
            "Epoch: 266, Loss: 0.000398\n",
            "Epoch: 267, Loss: 0.000138\n",
            "Epoch: 268, Loss: 0.000278\n",
            "Epoch: 269, Loss: 0.000246\n",
            "Epoch: 270, Loss: 0.000149\n",
            "Epoch: 271, Loss: 0.000160\n",
            "Epoch: 272, Loss: 0.000240\n",
            "Epoch: 273, Loss: 0.000332\n",
            "Epoch: 274, Loss: 0.000155\n",
            "Epoch: 275, Loss: 0.000149\n",
            "Epoch: 276, Loss: 0.000241\n",
            "Epoch: 277, Loss: 0.000313\n",
            "Epoch: 278, Loss: 0.000294\n",
            "Epoch: 279, Loss: 0.000340\n",
            "Epoch: 280, Loss: 0.000374\n",
            "Epoch: 281, Loss: 0.000179\n",
            "Epoch: 282, Loss: 0.000137\n",
            "Epoch: 283, Loss: 0.000193\n",
            "Epoch: 284, Loss: 0.000123\n",
            "Epoch: 285, Loss: 0.000215\n",
            "Epoch: 286, Loss: 0.000221\n",
            "Epoch: 287, Loss: 0.000345\n",
            "Epoch: 288, Loss: 0.000269\n",
            "Epoch: 289, Loss: 0.000242\n",
            "Epoch: 290, Loss: 0.000326\n",
            "Epoch: 291, Loss: 0.000323\n",
            "Epoch: 292, Loss: 0.000298\n",
            "Epoch: 293, Loss: 0.000178\n",
            "Epoch: 294, Loss: 0.000225\n",
            "Epoch: 295, Loss: 0.000326\n",
            "Epoch: 296, Loss: 0.000264\n",
            "Epoch: 297, Loss: 0.000198\n",
            "Epoch: 298, Loss: 0.000148\n",
            "Epoch: 299, Loss: 0.000240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_p1b.state_dict(), data_path + '/problem_1b_model.pt')"
      ],
      "metadata": {
        "id": "sTGJIHaE-keW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                           shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = model_p1b(imgs.view(imgs.shape[0], -1))\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "print(\"Accuracy: %f\" % (correct / total))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1D3lJEWhCvQ",
        "outputId": "618734fd-90cf-4b9b-c635-2d26c29a3190"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.474200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3cW4vYl-hTSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2"
      ],
      "metadata": {
        "id": "U-mFuYizhV2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = (torch.device('cuda') if torch.cuda.is_available()\n",
        "          else torch.device('cpu'))"
      ],
      "metadata": {
        "id": "HOMQbzpOhYUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sey-g7ggmebE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "\n",
        "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        loss_train = 0.0\n",
        "        for imgs, labels in train_loader:\n",
        "            imgs = imgs.to(device=device)  # <1>\n",
        "            labels = labels.to(device=device)\n",
        "            outputs = model(imgs)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_train += loss.item()\n",
        "\n",
        "        if epoch == 1 or epoch % 10 == 0:\n",
        "            print('{} Epoch {}, Training loss {}'.format(\n",
        "                datetime.datetime.now(), epoch,\n",
        "                loss_train / len(train_loader)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50pGq3qBkq1i",
        "outputId": "7eef38e4-84e0-4fcb-acf6-24ad8c6607db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n"
      ],
      "metadata": {
        "id": "RamIhf-LmcOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
        "                                           shuffle=True)\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = out.view(-1, 8 * 8 * 8)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "    \n",
        "model = Net()\n",
        "\n",
        "learning_rate = 1e-2\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "n_epochs = 300\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = model(imgs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "                \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2shRzOZiukw1",
        "outputId": "969ec7e3-072d-4cd2-ab60-f7af9e8b16f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 1.696354\n",
            "Epoch: 1, Loss: 1.559143\n",
            "Epoch: 2, Loss: 1.366394\n",
            "Epoch: 3, Loss: 1.360212\n",
            "Epoch: 4, Loss: 1.635762\n",
            "Epoch: 5, Loss: 1.287483\n",
            "Epoch: 6, Loss: 0.963239\n",
            "Epoch: 7, Loss: 1.145380\n",
            "Epoch: 8, Loss: 1.205747\n",
            "Epoch: 9, Loss: 1.390576\n",
            "Epoch: 10, Loss: 1.333235\n",
            "Epoch: 11, Loss: 0.970051\n",
            "Epoch: 12, Loss: 1.062223\n",
            "Epoch: 13, Loss: 0.738987\n",
            "Epoch: 14, Loss: 1.416537\n",
            "Epoch: 15, Loss: 0.962610\n",
            "Epoch: 16, Loss: 1.311636\n",
            "Epoch: 17, Loss: 1.116247\n",
            "Epoch: 18, Loss: 0.856801\n",
            "Epoch: 19, Loss: 1.320915\n",
            "Epoch: 20, Loss: 1.225533\n",
            "Epoch: 21, Loss: 0.688690\n",
            "Epoch: 22, Loss: 0.806198\n",
            "Epoch: 23, Loss: 0.874920\n",
            "Epoch: 24, Loss: 0.824794\n",
            "Epoch: 25, Loss: 0.967564\n",
            "Epoch: 26, Loss: 1.209329\n",
            "Epoch: 27, Loss: 1.067563\n",
            "Epoch: 28, Loss: 1.137812\n",
            "Epoch: 29, Loss: 0.820350\n",
            "Epoch: 30, Loss: 0.948253\n",
            "Epoch: 31, Loss: 0.741987\n",
            "Epoch: 32, Loss: 1.084160\n",
            "Epoch: 33, Loss: 1.037233\n",
            "Epoch: 34, Loss: 0.665583\n",
            "Epoch: 35, Loss: 0.554407\n",
            "Epoch: 36, Loss: 0.666157\n",
            "Epoch: 37, Loss: 0.906859\n",
            "Epoch: 38, Loss: 0.999518\n",
            "Epoch: 39, Loss: 0.645707\n",
            "Epoch: 40, Loss: 0.903674\n",
            "Epoch: 41, Loss: 0.756380\n",
            "Epoch: 42, Loss: 0.607007\n",
            "Epoch: 43, Loss: 1.006820\n",
            "Epoch: 44, Loss: 0.907203\n",
            "Epoch: 45, Loss: 1.523805\n",
            "Epoch: 46, Loss: 0.879318\n",
            "Epoch: 47, Loss: 0.791477\n",
            "Epoch: 48, Loss: 0.990526\n",
            "Epoch: 49, Loss: 0.688645\n",
            "Epoch: 50, Loss: 0.450428\n",
            "Epoch: 51, Loss: 0.755623\n",
            "Epoch: 52, Loss: 0.724029\n",
            "Epoch: 53, Loss: 0.937065\n",
            "Epoch: 54, Loss: 0.516842\n",
            "Epoch: 55, Loss: 0.432211\n",
            "Epoch: 56, Loss: 0.766732\n",
            "Epoch: 57, Loss: 1.348113\n",
            "Epoch: 58, Loss: 0.837253\n",
            "Epoch: 59, Loss: 1.079545\n",
            "Epoch: 60, Loss: 0.723401\n",
            "Epoch: 61, Loss: 1.199217\n",
            "Epoch: 62, Loss: 1.235191\n",
            "Epoch: 63, Loss: 0.710519\n",
            "Epoch: 64, Loss: 0.672011\n",
            "Epoch: 65, Loss: 0.612458\n",
            "Epoch: 66, Loss: 0.855486\n",
            "Epoch: 67, Loss: 0.913652\n",
            "Epoch: 68, Loss: 1.081235\n",
            "Epoch: 69, Loss: 0.892412\n",
            "Epoch: 70, Loss: 1.007644\n",
            "Epoch: 71, Loss: 0.876561\n",
            "Epoch: 72, Loss: 0.711199\n",
            "Epoch: 73, Loss: 0.692152\n",
            "Epoch: 74, Loss: 0.619192\n",
            "Epoch: 75, Loss: 0.654262\n",
            "Epoch: 76, Loss: 0.438120\n",
            "Epoch: 77, Loss: 0.833586\n",
            "Epoch: 78, Loss: 0.827599\n",
            "Epoch: 79, Loss: 0.856838\n",
            "Epoch: 80, Loss: 0.591354\n",
            "Epoch: 81, Loss: 0.538892\n",
            "Epoch: 82, Loss: 0.718286\n",
            "Epoch: 83, Loss: 0.518403\n",
            "Epoch: 84, Loss: 0.549424\n",
            "Epoch: 85, Loss: 0.826528\n",
            "Epoch: 86, Loss: 0.934117\n",
            "Epoch: 87, Loss: 0.656191\n",
            "Epoch: 88, Loss: 0.474820\n",
            "Epoch: 89, Loss: 1.397061\n",
            "Epoch: 90, Loss: 0.575001\n",
            "Epoch: 91, Loss: 0.877729\n",
            "Epoch: 92, Loss: 0.263518\n",
            "Epoch: 93, Loss: 0.560585\n",
            "Epoch: 94, Loss: 0.956584\n",
            "Epoch: 95, Loss: 0.283693\n",
            "Epoch: 96, Loss: 0.479215\n",
            "Epoch: 97, Loss: 0.682102\n",
            "Epoch: 98, Loss: 0.739320\n",
            "Epoch: 99, Loss: 0.854261\n",
            "Epoch: 100, Loss: 0.736392\n",
            "Epoch: 101, Loss: 0.955221\n",
            "Epoch: 102, Loss: 0.450004\n",
            "Epoch: 103, Loss: 0.410647\n",
            "Epoch: 104, Loss: 0.696778\n",
            "Epoch: 105, Loss: 0.683255\n",
            "Epoch: 106, Loss: 0.968980\n",
            "Epoch: 107, Loss: 0.363009\n",
            "Epoch: 108, Loss: 1.048256\n",
            "Epoch: 109, Loss: 0.452704\n",
            "Epoch: 110, Loss: 1.053886\n",
            "Epoch: 111, Loss: 0.926851\n",
            "Epoch: 112, Loss: 1.198118\n",
            "Epoch: 113, Loss: 0.668558\n",
            "Epoch: 114, Loss: 0.331472\n",
            "Epoch: 115, Loss: 0.320469\n",
            "Epoch: 116, Loss: 0.221149\n",
            "Epoch: 117, Loss: 0.642884\n",
            "Epoch: 118, Loss: 0.539818\n",
            "Epoch: 119, Loss: 0.598075\n",
            "Epoch: 120, Loss: 0.777134\n",
            "Epoch: 121, Loss: 0.443850\n",
            "Epoch: 122, Loss: 0.564584\n",
            "Epoch: 123, Loss: 0.731724\n",
            "Epoch: 124, Loss: 0.795400\n",
            "Epoch: 125, Loss: 0.418385\n",
            "Epoch: 126, Loss: 0.455493\n",
            "Epoch: 127, Loss: 0.638263\n",
            "Epoch: 128, Loss: 0.756317\n",
            "Epoch: 129, Loss: 0.258665\n",
            "Epoch: 130, Loss: 0.467264\n",
            "Epoch: 131, Loss: 0.750455\n",
            "Epoch: 132, Loss: 0.429427\n",
            "Epoch: 133, Loss: 0.453529\n",
            "Epoch: 134, Loss: 0.520712\n",
            "Epoch: 135, Loss: 0.380541\n",
            "Epoch: 136, Loss: 0.756816\n",
            "Epoch: 137, Loss: 0.630433\n",
            "Epoch: 138, Loss: 0.526761\n",
            "Epoch: 139, Loss: 0.779123\n",
            "Epoch: 140, Loss: 0.771383\n",
            "Epoch: 141, Loss: 0.391466\n",
            "Epoch: 142, Loss: 0.381486\n",
            "Epoch: 143, Loss: 0.251616\n",
            "Epoch: 144, Loss: 0.397334\n",
            "Epoch: 145, Loss: 0.435780\n",
            "Epoch: 146, Loss: 0.590627\n",
            "Epoch: 147, Loss: 0.696341\n",
            "Epoch: 148, Loss: 0.536505\n",
            "Epoch: 149, Loss: 0.587359\n",
            "Epoch: 150, Loss: 0.549691\n",
            "Epoch: 151, Loss: 0.560061\n",
            "Epoch: 152, Loss: 0.647846\n",
            "Epoch: 153, Loss: 0.448704\n",
            "Epoch: 154, Loss: 0.483334\n",
            "Epoch: 155, Loss: 0.984099\n",
            "Epoch: 156, Loss: 0.654855\n",
            "Epoch: 157, Loss: 0.547575\n",
            "Epoch: 158, Loss: 0.590253\n",
            "Epoch: 159, Loss: 0.683305\n",
            "Epoch: 160, Loss: 0.526037\n",
            "Epoch: 161, Loss: 0.935711\n",
            "Epoch: 162, Loss: 0.515298\n",
            "Epoch: 163, Loss: 0.665274\n",
            "Epoch: 164, Loss: 0.740911\n",
            "Epoch: 165, Loss: 0.662350\n",
            "Epoch: 166, Loss: 0.633215\n",
            "Epoch: 167, Loss: 0.728890\n",
            "Epoch: 168, Loss: 0.845887\n",
            "Epoch: 169, Loss: 0.223135\n",
            "Epoch: 170, Loss: 0.607056\n",
            "Epoch: 171, Loss: 0.352546\n",
            "Epoch: 172, Loss: 0.799721\n",
            "Epoch: 173, Loss: 0.930210\n",
            "Epoch: 174, Loss: 0.298904\n",
            "Epoch: 175, Loss: 0.620548\n",
            "Epoch: 176, Loss: 0.532530\n",
            "Epoch: 177, Loss: 0.789176\n",
            "Epoch: 178, Loss: 0.714655\n",
            "Epoch: 179, Loss: 0.542187\n",
            "Epoch: 180, Loss: 0.396839\n",
            "Epoch: 181, Loss: 0.346797\n",
            "Epoch: 182, Loss: 0.797740\n",
            "Epoch: 183, Loss: 0.592708\n",
            "Epoch: 184, Loss: 0.590084\n",
            "Epoch: 185, Loss: 0.693517\n",
            "Epoch: 186, Loss: 0.449156\n",
            "Epoch: 187, Loss: 0.617431\n",
            "Epoch: 188, Loss: 0.387034\n",
            "Epoch: 189, Loss: 0.937094\n",
            "Epoch: 190, Loss: 0.834180\n",
            "Epoch: 191, Loss: 0.487123\n",
            "Epoch: 192, Loss: 0.292121\n",
            "Epoch: 193, Loss: 0.732166\n",
            "Epoch: 194, Loss: 0.532336\n",
            "Epoch: 195, Loss: 0.385926\n",
            "Epoch: 196, Loss: 0.527318\n",
            "Epoch: 197, Loss: 0.558207\n",
            "Epoch: 198, Loss: 0.517853\n",
            "Epoch: 199, Loss: 0.848200\n",
            "Epoch: 200, Loss: 0.254769\n",
            "Epoch: 201, Loss: 0.568784\n",
            "Epoch: 202, Loss: 1.109579\n",
            "Epoch: 203, Loss: 0.538798\n",
            "Epoch: 204, Loss: 0.446159\n",
            "Epoch: 205, Loss: 0.657378\n",
            "Epoch: 206, Loss: 1.056101\n",
            "Epoch: 207, Loss: 0.405621\n",
            "Epoch: 208, Loss: 0.794464\n",
            "Epoch: 209, Loss: 0.423895\n",
            "Epoch: 210, Loss: 0.609485\n",
            "Epoch: 211, Loss: 0.570596\n",
            "Epoch: 212, Loss: 0.269970\n",
            "Epoch: 213, Loss: 0.710739\n",
            "Epoch: 214, Loss: 0.579787\n",
            "Epoch: 215, Loss: 0.207617\n",
            "Epoch: 216, Loss: 0.930185\n",
            "Epoch: 217, Loss: 0.473383\n",
            "Epoch: 218, Loss: 0.251750\n",
            "Epoch: 219, Loss: 0.527932\n",
            "Epoch: 220, Loss: 0.399807\n",
            "Epoch: 221, Loss: 0.569664\n",
            "Epoch: 222, Loss: 0.925527\n",
            "Epoch: 223, Loss: 0.489867\n",
            "Epoch: 224, Loss: 1.206827\n",
            "Epoch: 225, Loss: 0.682608\n",
            "Epoch: 226, Loss: 0.595119\n",
            "Epoch: 227, Loss: 0.524586\n",
            "Epoch: 228, Loss: 0.721702\n",
            "Epoch: 229, Loss: 0.429519\n",
            "Epoch: 230, Loss: 0.439714\n",
            "Epoch: 231, Loss: 0.222024\n",
            "Epoch: 232, Loss: 1.423804\n",
            "Epoch: 233, Loss: 0.319800\n",
            "Epoch: 234, Loss: 0.558257\n",
            "Epoch: 235, Loss: 0.603790\n",
            "Epoch: 236, Loss: 0.405610\n",
            "Epoch: 237, Loss: 0.393622\n",
            "Epoch: 238, Loss: 0.490123\n",
            "Epoch: 239, Loss: 0.443574\n",
            "Epoch: 240, Loss: 0.356219\n",
            "Epoch: 241, Loss: 0.287457\n",
            "Epoch: 242, Loss: 0.465650\n",
            "Epoch: 243, Loss: 0.335904\n",
            "Epoch: 244, Loss: 0.667378\n",
            "Epoch: 245, Loss: 0.427751\n",
            "Epoch: 246, Loss: 0.754195\n",
            "Epoch: 247, Loss: 0.805121\n",
            "Epoch: 248, Loss: 0.285297\n",
            "Epoch: 249, Loss: 0.354077\n",
            "Epoch: 250, Loss: 0.837324\n",
            "Epoch: 251, Loss: 0.359474\n",
            "Epoch: 252, Loss: 0.756047\n",
            "Epoch: 253, Loss: 0.541247\n",
            "Epoch: 254, Loss: 0.897637\n",
            "Epoch: 255, Loss: 0.220892\n",
            "Epoch: 256, Loss: 0.694510\n",
            "Epoch: 257, Loss: 0.418360\n",
            "Epoch: 258, Loss: 0.354231\n",
            "Epoch: 259, Loss: 0.356831\n",
            "Epoch: 260, Loss: 0.287072\n",
            "Epoch: 261, Loss: 0.498105\n",
            "Epoch: 262, Loss: 0.216502\n",
            "Epoch: 263, Loss: 0.676482\n",
            "Epoch: 264, Loss: 0.774882\n",
            "Epoch: 265, Loss: 0.551803\n",
            "Epoch: 266, Loss: 0.393642\n",
            "Epoch: 267, Loss: 0.715515\n",
            "Epoch: 268, Loss: 0.754526\n",
            "Epoch: 269, Loss: 0.356931\n",
            "Epoch: 270, Loss: 0.279171\n",
            "Epoch: 271, Loss: 1.343893\n",
            "Epoch: 272, Loss: 0.738171\n",
            "Epoch: 273, Loss: 0.776372\n",
            "Epoch: 274, Loss: 0.793222\n",
            "Epoch: 275, Loss: 0.480779\n",
            "Epoch: 276, Loss: 0.459406\n",
            "Epoch: 277, Loss: 0.367891\n",
            "Epoch: 278, Loss: 0.919785\n",
            "Epoch: 279, Loss: 0.482758\n",
            "Epoch: 280, Loss: 0.419480\n",
            "Epoch: 281, Loss: 0.557097\n",
            "Epoch: 282, Loss: 0.416792\n",
            "Epoch: 283, Loss: 0.616923\n",
            "Epoch: 284, Loss: 0.433218\n",
            "Epoch: 285, Loss: 0.452435\n",
            "Epoch: 286, Loss: 0.904533\n",
            "Epoch: 287, Loss: 1.017855\n",
            "Epoch: 288, Loss: 0.430097\n",
            "Epoch: 289, Loss: 0.987030\n",
            "Epoch: 290, Loss: 0.997141\n",
            "Epoch: 291, Loss: 0.885850\n",
            "Epoch: 292, Loss: 0.282614\n",
            "Epoch: 293, Loss: 0.710773\n",
            "Epoch: 294, Loss: 0.617859\n",
            "Epoch: 295, Loss: 0.250259\n",
            "Epoch: 296, Loss: 0.639932\n",
            "Epoch: 297, Loss: 0.736298\n",
            "Epoch: 298, Loss: 0.572747\n",
            "Epoch: 299, Loss: 0.371904\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), data_path + '/problem_2b_model.pt')"
      ],
      "metadata": {
        "id": "3JPcSAicJy2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = out.view(-1, 8 * 8 * 8)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "loaded_model = Net()\n",
        "loaded_model.load_state_dict(torch.load(data_path + '/problem_2b_model.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anIB54OlDhUP",
        "outputId": "c8cf3d46-0ba1-42dd-9981-be28c149cf23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        outputs = loaded_model(imgs)\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "        \n",
        "print(\"Accuracy: %f\" % (correct / total))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2I6UqvREkYT",
        "outputId": "413ca2f4-bfb1-4628-f035-25999a988c83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.627100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = (torch.device('cuda') if torch.cuda.is_available()\n",
        "else torch.device('cpu'))"
      ],
      "metadata": {
        "id": "AlUeUf17Luu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,shuffle=False)\n",
        "\n",
        "class Net2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3,16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(8, 4, kernel_size=3, padding=1)\n",
        "        \n",
        "        self.fc1 = nn.Linear(4 *4 *4, 32)\n",
        "        self.fc2 = nn.Linear(32, 10)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        out = F.max_pool2d(torch.tanh(self.conv1(x)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv2(out)), 2)\n",
        "        out = F.max_pool2d(torch.tanh(self.conv3(out)), 2)\n",
        "        out = out.view(-1, 4*4*4)\n",
        "        out = torch.tanh(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "model2b = Net2()\n",
        "\n",
        "learning_rate = 1e-2\n",
        "\n",
        "optimizer = optim.SGD(model2b.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "n_epochs = 300\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    for imgs, labels in train_loader:\n",
        "        outputs = model2b(imgs)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "                \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(\"Epoch: %d, Loss: %f\" % (epoch, float(loss)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vM5dDZy3FKfA",
        "outputId": "341e3dd1-2391-4a11-e3b6-0cc2fd5a85c8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Loss: 2.150151\n",
            "Epoch: 1, Loss: 2.051671\n",
            "Epoch: 2, Loss: 1.927019\n",
            "Epoch: 3, Loss: 1.732248\n",
            "Epoch: 4, Loss: 1.721456\n",
            "Epoch: 5, Loss: 1.759348\n",
            "Epoch: 6, Loss: 1.762437\n",
            "Epoch: 7, Loss: 1.735308\n",
            "Epoch: 8, Loss: 1.705194\n",
            "Epoch: 9, Loss: 1.698881\n",
            "Epoch: 10, Loss: 1.705467\n",
            "Epoch: 11, Loss: 1.689805\n",
            "Epoch: 12, Loss: 1.673851\n",
            "Epoch: 13, Loss: 1.673771\n",
            "Epoch: 14, Loss: 1.681946\n",
            "Epoch: 15, Loss: 1.678113\n",
            "Epoch: 16, Loss: 1.664534\n",
            "Epoch: 17, Loss: 1.632087\n",
            "Epoch: 18, Loss: 1.583084\n",
            "Epoch: 19, Loss: 1.523757\n",
            "Epoch: 20, Loss: 1.477598\n",
            "Epoch: 21, Loss: 1.429985\n",
            "Epoch: 22, Loss: 1.404349\n",
            "Epoch: 23, Loss: 1.380193\n",
            "Epoch: 24, Loss: 1.367446\n",
            "Epoch: 25, Loss: 1.356354\n",
            "Epoch: 26, Loss: 1.352082\n",
            "Epoch: 27, Loss: 1.344150\n",
            "Epoch: 28, Loss: 1.334642\n",
            "Epoch: 29, Loss: 1.330939\n",
            "Epoch: 30, Loss: 1.335567\n",
            "Epoch: 31, Loss: 1.337549\n",
            "Epoch: 32, Loss: 1.335122\n",
            "Epoch: 33, Loss: 1.333501\n",
            "Epoch: 34, Loss: 1.337295\n",
            "Epoch: 35, Loss: 1.343485\n",
            "Epoch: 36, Loss: 1.343097\n",
            "Epoch: 37, Loss: 1.342513\n",
            "Epoch: 38, Loss: 1.345510\n",
            "Epoch: 39, Loss: 1.339139\n",
            "Epoch: 40, Loss: 1.344444\n",
            "Epoch: 41, Loss: 1.343494\n",
            "Epoch: 42, Loss: 1.346372\n",
            "Epoch: 43, Loss: 1.340140\n",
            "Epoch: 44, Loss: 1.336827\n",
            "Epoch: 45, Loss: 1.339813\n",
            "Epoch: 46, Loss: 1.340259\n",
            "Epoch: 47, Loss: 1.338321\n",
            "Epoch: 48, Loss: 1.331117\n",
            "Epoch: 49, Loss: 1.329777\n",
            "Epoch: 50, Loss: 1.326302\n",
            "Epoch: 51, Loss: 1.320772\n",
            "Epoch: 52, Loss: 1.316364\n",
            "Epoch: 53, Loss: 1.321355\n",
            "Epoch: 54, Loss: 1.327846\n",
            "Epoch: 55, Loss: 1.326246\n",
            "Epoch: 56, Loss: 1.327920\n",
            "Epoch: 57, Loss: 1.320798\n",
            "Epoch: 58, Loss: 1.326118\n",
            "Epoch: 59, Loss: 1.322420\n",
            "Epoch: 60, Loss: 1.314037\n",
            "Epoch: 61, Loss: 1.309065\n",
            "Epoch: 62, Loss: 1.306688\n",
            "Epoch: 63, Loss: 1.299944\n",
            "Epoch: 64, Loss: 1.287231\n",
            "Epoch: 65, Loss: 1.280312\n",
            "Epoch: 66, Loss: 1.271811\n",
            "Epoch: 67, Loss: 1.273185\n",
            "Epoch: 68, Loss: 1.257502\n",
            "Epoch: 69, Loss: 1.253132\n",
            "Epoch: 70, Loss: 1.250332\n",
            "Epoch: 71, Loss: 1.249176\n",
            "Epoch: 72, Loss: 1.259690\n",
            "Epoch: 73, Loss: 1.252380\n",
            "Epoch: 74, Loss: 1.249269\n",
            "Epoch: 75, Loss: 1.246742\n",
            "Epoch: 76, Loss: 1.248517\n",
            "Epoch: 77, Loss: 1.248586\n",
            "Epoch: 78, Loss: 1.251439\n",
            "Epoch: 79, Loss: 1.252350\n",
            "Epoch: 80, Loss: 1.252543\n",
            "Epoch: 81, Loss: 1.246375\n",
            "Epoch: 82, Loss: 1.239718\n",
            "Epoch: 83, Loss: 1.237361\n",
            "Epoch: 84, Loss: 1.234210\n",
            "Epoch: 85, Loss: 1.234272\n",
            "Epoch: 86, Loss: 1.231271\n",
            "Epoch: 87, Loss: 1.231121\n",
            "Epoch: 88, Loss: 1.232465\n",
            "Epoch: 89, Loss: 1.229230\n",
            "Epoch: 90, Loss: 1.226072\n",
            "Epoch: 91, Loss: 1.221248\n",
            "Epoch: 92, Loss: 1.220051\n",
            "Epoch: 93, Loss: 1.213209\n",
            "Epoch: 94, Loss: 1.204527\n",
            "Epoch: 95, Loss: 1.192958\n",
            "Epoch: 96, Loss: 1.184477\n",
            "Epoch: 97, Loss: 1.175140\n",
            "Epoch: 98, Loss: 1.172984\n",
            "Epoch: 99, Loss: 1.167199\n",
            "Epoch: 100, Loss: 1.162924\n",
            "Epoch: 101, Loss: 1.157534\n",
            "Epoch: 102, Loss: 1.153565\n",
            "Epoch: 103, Loss: 1.151842\n",
            "Epoch: 104, Loss: 1.145294\n",
            "Epoch: 105, Loss: 1.138168\n",
            "Epoch: 106, Loss: 1.134368\n",
            "Epoch: 107, Loss: 1.133744\n",
            "Epoch: 108, Loss: 1.129276\n",
            "Epoch: 109, Loss: 1.129758\n",
            "Epoch: 110, Loss: 1.127809\n",
            "Epoch: 111, Loss: 1.125450\n",
            "Epoch: 112, Loss: 1.125687\n",
            "Epoch: 113, Loss: 1.121423\n",
            "Epoch: 114, Loss: 1.122013\n",
            "Epoch: 115, Loss: 1.116625\n",
            "Epoch: 116, Loss: 1.113856\n",
            "Epoch: 117, Loss: 1.113598\n",
            "Epoch: 118, Loss: 1.110374\n",
            "Epoch: 119, Loss: 1.109014\n",
            "Epoch: 120, Loss: 1.102585\n",
            "Epoch: 121, Loss: 1.102664\n",
            "Epoch: 122, Loss: 1.101304\n",
            "Epoch: 123, Loss: 1.098431\n",
            "Epoch: 124, Loss: 1.091135\n",
            "Epoch: 125, Loss: 1.087788\n",
            "Epoch: 126, Loss: 1.085169\n",
            "Epoch: 127, Loss: 1.080948\n",
            "Epoch: 128, Loss: 1.078761\n",
            "Epoch: 129, Loss: 1.073989\n",
            "Epoch: 130, Loss: 1.074954\n",
            "Epoch: 131, Loss: 1.072495\n",
            "Epoch: 132, Loss: 1.069018\n",
            "Epoch: 133, Loss: 1.072585\n",
            "Epoch: 134, Loss: 1.068617\n",
            "Epoch: 135, Loss: 1.067371\n",
            "Epoch: 136, Loss: 1.059730\n",
            "Epoch: 137, Loss: 1.064497\n",
            "Epoch: 138, Loss: 1.057755\n",
            "Epoch: 139, Loss: 1.062166\n",
            "Epoch: 140, Loss: 1.057311\n",
            "Epoch: 141, Loss: 1.056805\n",
            "Epoch: 142, Loss: 1.052297\n",
            "Epoch: 143, Loss: 1.052209\n",
            "Epoch: 144, Loss: 1.048383\n",
            "Epoch: 145, Loss: 1.039845\n",
            "Epoch: 146, Loss: 1.041413\n",
            "Epoch: 147, Loss: 1.042279\n",
            "Epoch: 148, Loss: 1.041225\n",
            "Epoch: 149, Loss: 1.037374\n",
            "Epoch: 150, Loss: 1.030425\n",
            "Epoch: 151, Loss: 1.024987\n",
            "Epoch: 152, Loss: 1.023516\n",
            "Epoch: 153, Loss: 1.018255\n",
            "Epoch: 154, Loss: 1.015562\n",
            "Epoch: 155, Loss: 1.012049\n",
            "Epoch: 156, Loss: 1.005807\n",
            "Epoch: 157, Loss: 1.006959\n",
            "Epoch: 158, Loss: 1.002213\n",
            "Epoch: 159, Loss: 0.998514\n",
            "Epoch: 160, Loss: 0.995081\n",
            "Epoch: 161, Loss: 0.991556\n",
            "Epoch: 162, Loss: 0.989650\n",
            "Epoch: 163, Loss: 0.983923\n",
            "Epoch: 164, Loss: 0.986306\n",
            "Epoch: 165, Loss: 0.986067\n",
            "Epoch: 166, Loss: 0.980743\n",
            "Epoch: 167, Loss: 0.977976\n",
            "Epoch: 168, Loss: 0.978828\n",
            "Epoch: 169, Loss: 0.976196\n",
            "Epoch: 170, Loss: 0.968299\n",
            "Epoch: 171, Loss: 0.973957\n",
            "Epoch: 172, Loss: 0.966017\n",
            "Epoch: 173, Loss: 0.966964\n",
            "Epoch: 174, Loss: 0.965358\n",
            "Epoch: 175, Loss: 0.962725\n",
            "Epoch: 176, Loss: 0.965713\n",
            "Epoch: 177, Loss: 0.966361\n",
            "Epoch: 178, Loss: 0.970777\n",
            "Epoch: 179, Loss: 0.964597\n",
            "Epoch: 180, Loss: 0.966889\n",
            "Epoch: 181, Loss: 0.962688\n",
            "Epoch: 182, Loss: 0.964975\n",
            "Epoch: 183, Loss: 0.963371\n",
            "Epoch: 184, Loss: 0.962576\n",
            "Epoch: 185, Loss: 0.957034\n",
            "Epoch: 186, Loss: 0.953608\n",
            "Epoch: 187, Loss: 0.953411\n",
            "Epoch: 188, Loss: 0.958620\n",
            "Epoch: 189, Loss: 0.950905\n",
            "Epoch: 190, Loss: 0.957171\n",
            "Epoch: 191, Loss: 0.954648\n",
            "Epoch: 192, Loss: 0.954248\n",
            "Epoch: 193, Loss: 0.948326\n",
            "Epoch: 194, Loss: 0.949714\n",
            "Epoch: 195, Loss: 0.948945\n",
            "Epoch: 196, Loss: 0.952022\n",
            "Epoch: 197, Loss: 0.954012\n",
            "Epoch: 198, Loss: 0.948371\n",
            "Epoch: 199, Loss: 0.945536\n",
            "Epoch: 200, Loss: 0.943430\n",
            "Epoch: 201, Loss: 0.938642\n",
            "Epoch: 202, Loss: 0.932466\n",
            "Epoch: 203, Loss: 0.933290\n",
            "Epoch: 204, Loss: 0.931715\n",
            "Epoch: 205, Loss: 0.933271\n",
            "Epoch: 206, Loss: 0.929304\n",
            "Epoch: 207, Loss: 0.931082\n",
            "Epoch: 208, Loss: 0.935354\n",
            "Epoch: 209, Loss: 0.935671\n",
            "Epoch: 210, Loss: 0.936304\n",
            "Epoch: 211, Loss: 0.936910\n",
            "Epoch: 212, Loss: 0.935271\n",
            "Epoch: 213, Loss: 0.929501\n",
            "Epoch: 214, Loss: 0.930888\n",
            "Epoch: 215, Loss: 0.929977\n",
            "Epoch: 216, Loss: 0.925852\n",
            "Epoch: 217, Loss: 0.927345\n",
            "Epoch: 218, Loss: 0.929229\n",
            "Epoch: 219, Loss: 0.929176\n",
            "Epoch: 220, Loss: 0.927317\n",
            "Epoch: 221, Loss: 0.922224\n",
            "Epoch: 222, Loss: 0.928385\n",
            "Epoch: 223, Loss: 0.929464\n",
            "Epoch: 224, Loss: 0.929572\n",
            "Epoch: 225, Loss: 0.931170\n",
            "Epoch: 226, Loss: 0.930585\n",
            "Epoch: 227, Loss: 0.925975\n",
            "Epoch: 228, Loss: 0.923719\n",
            "Epoch: 229, Loss: 0.923093\n",
            "Epoch: 230, Loss: 0.922285\n",
            "Epoch: 231, Loss: 0.925579\n",
            "Epoch: 232, Loss: 0.927048\n",
            "Epoch: 233, Loss: 0.932557\n",
            "Epoch: 234, Loss: 0.930517\n",
            "Epoch: 235, Loss: 0.929505\n",
            "Epoch: 236, Loss: 0.927432\n",
            "Epoch: 237, Loss: 0.924219\n",
            "Epoch: 238, Loss: 0.924643\n",
            "Epoch: 239, Loss: 0.927945\n",
            "Epoch: 240, Loss: 0.929331\n",
            "Epoch: 241, Loss: 0.927883\n",
            "Epoch: 242, Loss: 0.929289\n",
            "Epoch: 243, Loss: 0.927479\n",
            "Epoch: 244, Loss: 0.927496\n",
            "Epoch: 245, Loss: 0.923824\n",
            "Epoch: 246, Loss: 0.919592\n",
            "Epoch: 247, Loss: 0.917680\n",
            "Epoch: 248, Loss: 0.920639\n",
            "Epoch: 249, Loss: 0.919066\n",
            "Epoch: 250, Loss: 0.918273\n",
            "Epoch: 251, Loss: 0.916816\n",
            "Epoch: 252, Loss: 0.918809\n",
            "Epoch: 253, Loss: 0.924244\n",
            "Epoch: 254, Loss: 0.922271\n",
            "Epoch: 255, Loss: 0.917295\n",
            "Epoch: 256, Loss: 0.921535\n",
            "Epoch: 257, Loss: 0.921423\n",
            "Epoch: 258, Loss: 0.920818\n",
            "Epoch: 259, Loss: 0.913766\n",
            "Epoch: 260, Loss: 0.913818\n",
            "Epoch: 261, Loss: 0.916351\n",
            "Epoch: 262, Loss: 0.915387\n",
            "Epoch: 263, Loss: 0.917894\n",
            "Epoch: 264, Loss: 0.918407\n",
            "Epoch: 265, Loss: 0.915791\n",
            "Epoch: 266, Loss: 0.912200\n",
            "Epoch: 267, Loss: 0.915849\n",
            "Epoch: 268, Loss: 0.914211\n",
            "Epoch: 269, Loss: 0.911929\n",
            "Epoch: 270, Loss: 0.914568\n",
            "Epoch: 271, Loss: 0.917192\n",
            "Epoch: 272, Loss: 0.910845\n",
            "Epoch: 273, Loss: 0.907614\n",
            "Epoch: 274, Loss: 0.910258\n",
            "Epoch: 275, Loss: 0.908720\n",
            "Epoch: 276, Loss: 0.910034\n",
            "Epoch: 277, Loss: 0.913738\n",
            "Epoch: 278, Loss: 0.911461\n",
            "Epoch: 279, Loss: 0.911467\n",
            "Epoch: 280, Loss: 0.908387\n",
            "Epoch: 281, Loss: 0.913184\n",
            "Epoch: 282, Loss: 0.913112\n",
            "Epoch: 283, Loss: 0.910701\n",
            "Epoch: 284, Loss: 0.911362\n",
            "Epoch: 285, Loss: 0.911539\n",
            "Epoch: 286, Loss: 0.915707\n",
            "Epoch: 287, Loss: 0.913878\n",
            "Epoch: 288, Loss: 0.913278\n",
            "Epoch: 289, Loss: 0.912519\n",
            "Epoch: 290, Loss: 0.913616\n",
            "Epoch: 291, Loss: 0.912829\n",
            "Epoch: 292, Loss: 0.909136\n",
            "Epoch: 293, Loss: 0.911780\n",
            "Epoch: 294, Loss: 0.915495\n",
            "Epoch: 295, Loss: 0.918946\n",
            "Epoch: 296, Loss: 0.922505\n",
            "Epoch: 297, Loss: 0.918938\n",
            "Epoch: 298, Loss: 0.918012\n",
            "Epoch: 299, Loss: 0.918703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model2b.state_dict(), data_path + '/problem_2a_model.pt')"
      ],
      "metadata": {
        "id": "e4rTCAj7R2T8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
        "                                         shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        outputs = model2b(imgs)\n",
        "        _, predicted = torch.max(outputs, dim=1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predicted == labels).sum())\n",
        "        \n",
        "print(\"Accuracy: %f\" % (correct / total))\n"
      ],
      "metadata": {
        "id": "0ppwAe-tQ0ws",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "237ea869-9d85-4ece-a5a2-ffb732d6b9f8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.630800\n"
          ]
        }
      ]
    }
  ]
}