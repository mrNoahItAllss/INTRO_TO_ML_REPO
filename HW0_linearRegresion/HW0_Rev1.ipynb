{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV \n",
    "df = pd.read_csv('D3.csv')\n",
    "# Initialize inputs outputs and the number of data points \n",
    "x_1 = df.values[:,0]\n",
    "x_2 = df.values[:,1]\n",
    "x_3 = df.values[:,2]\n",
    "Y = df.values[:,3]\n",
    "m = len(Y)\n",
    "x_0 = np.ones((m,1))\n",
    "# put in list for automation \n",
    "x_i_old = [x_1,x_2,x_3]\n",
    "x_i = [x_1,x_2,x_3]\n",
    "# Initialize Results \n",
    "final_thetas = np.zeros((3,2))\n",
    "costhistorys = [[]] * 3 \n",
    "# Initialize Value for terms to change\n",
    "alpha = .01\n",
    "iterations = 1500\n",
    "theta = np.zeros(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, theta): \n",
    "  \"\"\" \n",
    "  Compute cost for linear regression. \n",
    " \n",
    "  Input Parameters \n",
    "  ---------------- \n",
    "  X : 2D array where each row represent the training example and each column represent \n",
    "      m= number of training examples \n",
    "      n= number of features (including X_0 column of ones) \n",
    "  y : 1D array of labels/target value for each traing example. dimension(1 x m) \n",
    " \n",
    "  theta : 1D array of fitting parameters or weights. Dimension (1 x n) \n",
    " \n",
    "  Output Parameters \n",
    "  ----------------- \n",
    "  J : Scalar value. \n",
    "  \"\"\" \n",
    "  predictions = X.dot(theta) \n",
    "  errors = np.subtract(predictions, y) \n",
    "  sqrErrors = np.square(errors) \n",
    "  J = 1 / (2 * m) * np.sum(sqrErrors) \n",
    " \n",
    "  return J "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, iterations): \n",
    "  \"\"\" \n",
    "  Compute cost for linear regression. \n",
    " \n",
    "  Input Parameters \n",
    "  ---------------- \n",
    "  X : 2D array where each row represent the training example and each column represent \n",
    "      m= number of training examples \n",
    "      n= number of features (including X_0 column of ones) \n",
    "  y : 1D array of labels/target value for each traing example. dimension(m x 1) \n",
    "  theta : 1D array of fitting parameters or weights. Dimension (1 x n) \n",
    "  alpha : Learning rate. Scalar value \n",
    "  iterations: No of iterations. Scalar value.  \n",
    " \n",
    "  Output Parameters \n",
    "  ----------------- \n",
    "  theta : Final Value. 1D array of fitting parameters or weights. Dimension (1 x n) \n",
    "  cost_history: Conatins value of cost for each iteration. 1D array. Dimansion(m x 1)   \n",
    "  \"\"\" \n",
    "  cost_history = np.zeros(iterations) \n",
    " \n",
    "  for i in range(iterations): \n",
    "    predictions = X.dot(theta) \n",
    "    errors = np.subtract(predictions, y) \n",
    "    sum_delta = (alpha / m) * X.transpose().dot(errors); \n",
    "    theta = theta - sum_delta; \n",
    "    cost_history[i] = compute_cost(X, y, theta)   \n",
    " \n",
    "  return theta, cost_history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preform the linear regression then store \n",
    "for i in range(len(costhistorys)):\n",
    "    # Reshape for linear algebra \n",
    "    x_i[i] = x_i[i].reshape(m,1)\n",
    "    # Add x_0 term to each X value \n",
    "    x_i[i] = np.hstack((x_0,x_i[i]))\n",
    "    # preform the calculations\n",
    "    final_thetas[i,:], costhistorys[i] = gradient_descent(x_i[i], Y, theta, alpha, iterations)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model for x_0: [ 5.71850653 -1.9568206 ]\n",
      "Model for x_1: [0.71988473 0.56390334]\n",
      "Model for x_2: [ 2.78048129 -0.48451631]\n"
     ]
    }
   ],
   "source": [
    "for i in range(final_thetas.shape[0]):\n",
    "    print('Model for x_' + str(i) + ':',final_thetas[i,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9905894438682062"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cost(x_i[0], Y, final_thetas[0,:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 720x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Linear Regression and Cost Plot \n",
    "for i in range(len(x_i)):   \n",
    "    plt.scatter(x_i_old[i], Y, color='red', marker= '+', label= 'Training Data') \n",
    "    plt.plot(x_i[i][:,1], x_i[i].dot(final_thetas[i,:]), color='green', label='Linear Regression') \n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (10,6) \n",
    "    plt.grid() \n",
    "    plt.xlabel('x_' + str(i + 1) + ' Values') \n",
    "    plt.ylabel('Y Values') \n",
    "    plt.title('Linear Regression Fit') \n",
    "    plt.legend() \n",
    "    plt.savefig('output'+ str(i) + '.jpg')\n",
    "    plt.clf()\n",
    "    \n",
    "\n",
    "    plt.plot(range(1, iterations + 1),costhistorys[i], color='blue') \n",
    "    plt.rcParams[\"figure.figsize\"] = (10,6) \n",
    "    plt.grid() \n",
    "    plt.xlabel('Number of iterations') \n",
    "    plt.ylabel('Cost (J)') \n",
    "    plt.title('Convergence of gradient descent for x_' + str(i + 1))\n",
    "    plt.savefig('convergence' + str(i) + '.jpg') \n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('D3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Variables \n",
    "iterations2 = 1500\n",
    "alpha2 = .14\n",
    "# Add Column of ones to the begining of the data \n",
    "df2 =  pd.concat([pd.Series(1, index=df.index, name='x_0'), df], axis=1)\n",
    "# Get the inputs \n",
    "X = df2.drop(columns='Y')\n",
    "# Get the outputs \n",
    "Y = df2.values[:,4]\n",
    "# Declare theta \n",
    "theta = np.array([0]*len(X.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta_0 = 5.314167168826393\n",
      "Theta_1 = -2.003719268255627\n",
      "Theta_2 = 0.5325633367298653\n",
      "Theta_3 = -0.26560186498944977\n"
     ]
    }
   ],
   "source": [
    "# Run the linear regression algorithm\n",
    "model, cost_func = gradient_descent(X,Y,theta,alpha2,iterations2)\n",
    "temp = 0\n",
    "for i in model:\n",
    "    print('Theta_' + str(temp) + ' =', i)\n",
    "    temp += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 720x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(1, iterations2 + 1),cost_func, color='blue') \n",
    "plt.rcParams[\"figure.figsize\"] = (10,6) \n",
    "plt.grid() \n",
    "plt.xlabel('Number of iterations') \n",
    "plt.ylabel('Cost (J)') \n",
    "plt.title('Convergence of gradient descent for entire model')\n",
    "plt.savefig('convergence_part2.jpg') \n",
    "plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Value 1: 3.577409372311182\n",
      "Predicted Value 2: 0.2443211723573404\n",
      "Predicted Value 3: 0.10253417252979352\n"
     ]
    }
   ],
   "source": [
    "# Create New dataframe for new data \n",
    "data = {'x1New' : [1,2,3], 'x2New' : [1,0,2], 'x3New' : [1,4,1]}\n",
    "newDf = pd.DataFrame(data)\n",
    "newDf =  pd.concat([pd.Series(1, index=newDf.index, name='x0New'), newDf], axis=1)\n",
    "newDf = newDf.to_numpy() # Needs to be a numpy array to do the dot module\n",
    "# dot new data with model to predict the output\n",
    "Hypothosis = newDf.dot(model)\n",
    "for i in range(len(Hypothosis)):\n",
    "    print('Predicted Value ' + str(i+1) + ':', Hypothosis[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2d31b268c8d34cc6f5611edc2fbb7e7c104b3cc2bf760bd0cdc2d363100d4b87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
